{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"finetune_bart-base_transformers.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"bf9ba96fb6d54c22aaa4b665c237cd53":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","state":{"_view_name":"VBoxView","_dom_classes":[],"_model_name":"VBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_946af848f15f445ea5f9f9ac180afed7","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_5c476aed3eab46bdb53388f7a2cc19c5","IPY_MODEL_483cbaa9a76b42508c251ec87a33e652"]}},"946af848f15f445ea5f9f9ac180afed7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5c476aed3eab46bdb53388f7a2cc19c5":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","state":{"_view_name":"LabelView","style":"IPY_MODEL_d038493f153843069ca804c2abb32de9","_dom_classes":[],"description":"","_model_name":"LabelModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 0.01MB of 0.01MB uploaded (0.00MB deduped)\r","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1887333b9b374f3296b0a5b4da813e5f"}},"483cbaa9a76b42508c251ec87a33e652":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_b606b20a469b4183bda45bb96efd583f","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_080a7379e0f045ffa28ea1df954da91b"}},"d038493f153843069ca804c2abb32de9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"1887333b9b374f3296b0a5b4da813e5f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b606b20a469b4183bda45bb96efd583f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"080a7379e0f045ffa28ea1df954da91b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"05bd7145af8c409097837567e4ef6c53":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4cc3b00ff6b549a1a2d8c3d37c6d7877","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_68f1a84d7f7c437d93aa1104c4eaf120","IPY_MODEL_80f379f81dc647998fc9b17d1bbf6ec1"]}},"4cc3b00ff6b549a1a2d8c3d37c6d7877":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"68f1a84d7f7c437d93aa1104c4eaf120":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_6a24fd3f2a924a5cbd9bc2666859195c","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":557941479,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":557941479,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_156defbcd4e04920afbb389c9532494e"}},"80f379f81dc647998fc9b17d1bbf6ec1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4507446e4fe14be9ade46dd4f65b32b5","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 558M/558M [00:31&lt;00:00, 17.9MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3bd0375c30074aac960c27a913149966"}},"6a24fd3f2a924a5cbd9bc2666859195c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"156defbcd4e04920afbb389c9532494e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4507446e4fe14be9ade46dd4f65b32b5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3bd0375c30074aac960c27a913149966":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"1EDP7JKqzJyf"},"source":["# Fine Tuning Transformer for News Summarization\n"]},{"cell_type":"code","metadata":{"id":"WD_vnyLXZQzD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605732608166,"user_tz":480,"elapsed":16241,"user":{"displayName":"Nattapat Juthaprachakul","photoUrl":"","userId":"05256019876215417123"}},"outputId":"68d31e2b-013e-462a-e78c-935d6b14d68e"},"source":["# credit: https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb\n","!pip install transformers -q\n","!pip install wandb -q\n","\n","# Code for TPU packages install\n","# !curl -q https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n","# !python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev"],"execution_count":1,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 1.3MB 8.8MB/s \n","\u001b[K     |████████████████████████████████| 1.1MB 19.1MB/s \n","\u001b[K     |████████████████████████████████| 890kB 43.3MB/s \n","\u001b[K     |████████████████████████████████| 2.9MB 47.7MB/s \n","\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[K     |████████████████████████████████| 1.8MB 9.1MB/s \n","\u001b[K     |████████████████████████████████| 102kB 13.1MB/s \n","\u001b[K     |████████████████████████████████| 163kB 14.7MB/s \n","\u001b[K     |████████████████████████████████| 133kB 20.2MB/s \n","\u001b[K     |████████████████████████████████| 102kB 11.1MB/s \n","\u001b[K     |████████████████████████████████| 71kB 9.4MB/s \n","\u001b[?25h  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pzM1_ykHaFur","executionInfo":{"status":"ok","timestamp":1605732614439,"user_tz":480,"elapsed":18067,"user":{"displayName":"Nattapat Juthaprachakul","photoUrl":"","userId":"05256019876215417123"}}},"source":["# Importing stock libraries\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n","\n","# Importing the T5 modules from huggingface/transformers\n","# from transformers import T5Tokenizer, T5ForConditionalGeneration\n","\n","# WandB – Import the wandb library\n","import wandb"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KvPxXdKJguYB","executionInfo":{"status":"ok","timestamp":1605731258408,"user_tz":480,"elapsed":59428,"user":{"displayName":"Nattapat Juthaprachakul","photoUrl":"","userId":"05256019876215417123"}},"outputId":"40cf67f9-f3ce-4b28-c80c-d93a8e4d4d1d"},"source":["# Checking out the GPU we have access to. This is output is from the google colab version. \n","!nvidia-smi"],"execution_count":3,"outputs":[{"output_type":"stream","text":["NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NLxxwd1scQNv","executionInfo":{"status":"ok","timestamp":1605732614444,"user_tz":480,"elapsed":14047,"user":{"displayName":"Nattapat Juthaprachakul","photoUrl":"","userId":"05256019876215417123"}}},"source":["# # Setting up the device for GPU usage\n","from torch import cuda\n","device = 'cuda' if cuda.is_available() else 'cpu'\n","\n","# Preparing for TPU usage\n","# import torch_xla\n","# import torch_xla.core.xla_model as xm\n","# device = xm.xla_device()"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L-ePh9dEKXMw","executionInfo":{"status":"ok","timestamp":1605732739970,"user_tz":480,"elapsed":135762,"user":{"displayName":"Nattapat Juthaprachakul","photoUrl":"","userId":"05256019876215417123"}},"outputId":"ecabfec8-a4d5-47b1-d51b-2244c4d04e8b"},"source":["# Login to wandb to log the model run and all the parameters\n","!wandb login"],"execution_count":4,"outputs":[{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"932p8NhxeNw4","executionInfo":{"status":"ok","timestamp":1605732766922,"user_tz":480,"elapsed":400,"user":{"displayName":"Nattapat Juthaprachakul","photoUrl":"","userId":"05256019876215417123"}}},"source":["# Creating a custom dataset for reading the dataframe and loading it into the dataloader to pass it to the neural network at a later stage for finetuning the model and to prepare it for predictions\n","\n","class CustomDataset(Dataset):\n","\n","    def __init__(self, dataframe, tokenizer, source_len, summ_len):\n","        self.tokenizer = tokenizer\n","        self.data = dataframe\n","        self.source_len = source_len\n","        self.summ_len = summ_len\n","        self.text = self.data.text\n","        self.ctext = self.data.ctext\n","\n","    def __len__(self):\n","        return len(self.text)\n","\n","    def __getitem__(self, index):\n","        ctext = str(self.ctext[index])\n","        ctext = ' '.join(ctext.split())\n","\n","        text = str(self.text[index])\n","        text = ' '.join(text.split())\n","\n","        source = self.tokenizer.batch_encode_plus([ctext], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt')\n","        target = self.tokenizer.batch_encode_plus([text], max_length= self.summ_len, pad_to_max_length=True,return_tensors='pt')\n","\n","        source_ids = source['input_ids'].squeeze()\n","        source_mask = source['attention_mask'].squeeze()\n","        target_ids = target['input_ids'].squeeze()\n","        target_mask = target['attention_mask'].squeeze()\n","\n","        return {\n","            'source_ids': source_ids.to(dtype=torch.long), \n","            'source_mask': source_mask.to(dtype=torch.long), \n","            'target_ids': target_ids.to(dtype=torch.long),\n","            'target_ids_y': target_ids.to(dtype=torch.long)\n","        }"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"SaPAR7TWmxoM","executionInfo":{"status":"ok","timestamp":1605732769496,"user_tz":480,"elapsed":413,"user":{"displayName":"Nattapat Juthaprachakul","photoUrl":"","userId":"05256019876215417123"}}},"source":["# Creating the training function. This will be called in the main function. It is run depending on the epoch value.\n","# The model is put into train mode and then we wnumerate over the training loader and passed to the defined network \n","\n","def train(epoch, tokenizer, model, device, loader, optimizer):\n","    model.train()\n","    for _,data in enumerate(loader, 0):\n","        y = data['target_ids'].to(device, dtype = torch.long)\n","        y_ids = y[:, :-1].contiguous()\n","        lm_labels = y[:, 1:].clone().detach()\n","        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n","        ids = data['source_ids'].to(device, dtype = torch.long)\n","        mask = data['source_mask'].to(device, dtype = torch.long)\n","\n","        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, lm_labels=lm_labels)\n","        loss = outputs[0]\n","        \n","        if _%10 == 0:\n","            wandb.log({\"Training Loss\": loss.item()})\n","\n","        if _%500==0:\n","            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n","        \n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        # xm.optimizer_step(optimizer)\n","        # xm.mark_step()"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"j9TNdHlQ0CLz","executionInfo":{"status":"ok","timestamp":1605732772842,"user_tz":480,"elapsed":568,"user":{"displayName":"Nattapat Juthaprachakul","photoUrl":"","userId":"05256019876215417123"}}},"source":["def validate(epoch, tokenizer, model, device, loader):\n","    model.eval()\n","    predictions = []\n","    actuals = []\n","    with torch.no_grad():\n","        for _, data in enumerate(loader, 0):\n","            y = data['target_ids'].to(device, dtype = torch.long)\n","            ids = data['source_ids'].to(device, dtype = torch.long)\n","            mask = data['source_mask'].to(device, dtype = torch.long)\n","\n","            generated_ids = model.generate(\n","                input_ids = ids,\n","                attention_mask = mask, \n","                max_length=150, \n","                num_beams=2,\n","                repetition_penalty=2.5, \n","                length_penalty=1.0, \n","                early_stopping=True\n","                )\n","            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n","            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n","            if _%100==0:\n","                print(f'Completed {_}')\n","\n","            predictions.extend(preds)\n","            actuals.extend(target)\n","    return predictions, actuals"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hn8VMbXI-yyM","executionInfo":{"status":"ok","timestamp":1605732850843,"user_tz":480,"elapsed":30510,"user":{"displayName":"Nattapat Juthaprachakul","photoUrl":"","userId":"05256019876215417123"}},"outputId":"7450ab77-ce35-4ede-85d9-7494b3e69b2b"},"source":["import pickle\n","import copy\n","path_cnn= '/content/drive/My Drive/Colab Notebooks/data/train_dataset.pkl'\n","\n","stories = pickle.load(open(path_cnn, 'rb'))\n","print(\"total number of CNN data: \",len(stories))\n","\n","# number of highlights to be used\n","NUM_HIGHLIGHT= 1\n","# number of storie sentences to be used\n","NUM_STORY = 2\n","\n","# join sentence in both stories and highlights together for each data sample\n","processed_stories = copy.deepcopy(stories)\n","\n","# join stories and highlights into 2 column pd dataframe\n","for each_story in processed_stories:\n","  # join highlights\n","  each_story['highlights'] = ' '.join(each_story['highlights'][0:NUM_HIGHLIGHT])\n","  # join story sentences\n","  each_story['story'] = ' '.join(each_story['story'][0:NUM_STORY])\n"],"execution_count":14,"outputs":[{"output_type":"stream","text":["total number of CNN data:  301956\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":404},"id":"bum-mMwg-8fi","executionInfo":{"status":"ok","timestamp":1605732851097,"user_tz":480,"elapsed":28151,"user":{"displayName":"Nattapat Juthaprachakul","photoUrl":"","userId":"05256019876215417123"}},"outputId":"fd764b43-496b-4496-97df-520b91ec3a3d"},"source":["df_cnn = pd.DataFrame(processed_stories)\n","df_cnn['highlights'] = 'summarize: ' + df_cnn['highlights']\n","\n","df_cnn\n"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>story</th>\n","      <th>highlights</th>\n","      <th>id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>controversial malawi president bingu wa muthar...</td>\n","      <td>summarize: controversial leader collapsed on t...</td>\n","      <td>id_dm_143548</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>arizonans thats who it figures in the immigrat...</td>\n","      <td>summarize: arizona judge rules for group tryin...</td>\n","      <td>id_cnn_66720</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>northwestern universitys football players vote...</td>\n","      <td>summarize: northwestern university football pl...</td>\n","      <td>id_cnn_91222</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>a european union delegation met saturday with ...</td>\n","      <td>summarize: of eu meeting mugabe said there was...</td>\n","      <td>id_cnn_53005</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>world player of the year lionel messi grabbed ...</td>\n","      <td>summarize: lionel messi continues his incredib...</td>\n","      <td>id_cnn_21168</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>301951</th>\n","      <td>newcastle legend alan shearer has slammed the ...</td>\n","      <td>summarize: newcastle are currently in the prem...</td>\n","      <td>id_dm_175879</td>\n","    </tr>\n","    <tr>\n","      <th>301952</th>\n","      <td>the mayor of two romanian villages has claimed...</td>\n","      <td>summarize: remus neda is heading for the uk to...</td>\n","      <td>id_dm_29988</td>\n","    </tr>\n","    <tr>\n","      <th>301953</th>\n","      <td>the health of pope emeritus benedict xvi has d...</td>\n","      <td>summarize: respected vatican expert sparks fea...</td>\n","      <td>id_dm_162103</td>\n","    </tr>\n","    <tr>\n","      <th>301954</th>\n","      <td>like the moment that harry potter first learne...</td>\n","      <td>summarize: many ireporters feel a kinship to h...</td>\n","      <td>id_cnn_27915</td>\n","    </tr>\n","    <tr>\n","      <th>301955</th>\n","      <td>much of the country has turned up the heat dur...</td>\n","      <td>summarize: fastmoving cold front knifes throug...</td>\n","      <td>id_cnn_2259</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>301956 rows × 3 columns</p>\n","</div>"],"text/plain":["                                                    story  ...            id\n","0       controversial malawi president bingu wa muthar...  ...  id_dm_143548\n","1       arizonans thats who it figures in the immigrat...  ...  id_cnn_66720\n","2       northwestern universitys football players vote...  ...  id_cnn_91222\n","3       a european union delegation met saturday with ...  ...  id_cnn_53005\n","4       world player of the year lionel messi grabbed ...  ...  id_cnn_21168\n","...                                                   ...  ...           ...\n","301951  newcastle legend alan shearer has slammed the ...  ...  id_dm_175879\n","301952  the mayor of two romanian villages has claimed...  ...   id_dm_29988\n","301953  the health of pope emeritus benedict xvi has d...  ...  id_dm_162103\n","301954  like the moment that harry potter first learne...  ...  id_cnn_27915\n","301955  much of the country has turned up the heat dur...  ...   id_cnn_2259\n","\n","[301956 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"EDWlggCxJ2A6","executionInfo":{"status":"ok","timestamp":1605733219403,"user_tz":480,"elapsed":467,"user":{"displayName":"Nattapat Juthaprachakul","photoUrl":"","userId":"05256019876215417123"}}},"source":["from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":874,"referenced_widgets":["bf9ba96fb6d54c22aaa4b665c237cd53","946af848f15f445ea5f9f9ac180afed7","5c476aed3eab46bdb53388f7a2cc19c5","483cbaa9a76b42508c251ec87a33e652","d038493f153843069ca804c2abb32de9","1887333b9b374f3296b0a5b4da813e5f","b606b20a469b4183bda45bb96efd583f","080a7379e0f045ffa28ea1df954da91b","05bd7145af8c409097837567e4ef6c53","4cc3b00ff6b549a1a2d8c3d37c6d7877","68f1a84d7f7c437d93aa1104c4eaf120","80f379f81dc647998fc9b17d1bbf6ec1","6a24fd3f2a924a5cbd9bc2666859195c","156defbcd4e04920afbb389c9532494e","4507446e4fe14be9ade46dd4f65b32b5","3bd0375c30074aac960c27a913149966"]},"id":"ZtNs9ytpCow2","executionInfo":{"status":"error","timestamp":1605733324528,"user_tz":480,"elapsed":73646,"user":{"displayName":"Nattapat Juthaprachakul","photoUrl":"","userId":"05256019876215417123"}},"outputId":"c3642435-24f0-49e0-a842-a33d2522051b"},"source":["def main():\n","    # WandB – Initialize a new run\n","    wandb.init(project=\"finetune_transformers_summarization\")\n","\n","    # WandB – Config is a variable that holds and saves hyperparameters and inputs\n","    # Defining some key variables that will be used later on in the training  \n","    config = wandb.config          # Initialize config\n","    # config.TRAIN_BATCH_SIZE = 2    # input batch size for training (default: 64)\n","    config.TRAIN_BATCH_SIZE = 64    # input batch size for training (default: 64)\n","    # config.VALID_BATCH_SIZE = 2    # input batch size for testing (default: 1000)\n","    config.VALID_BATCH_SIZE = 1000    # input batch size for testing (default: 1000)\n","    config.TRAIN_EPOCHS = 2        # number of epochs to train (default: 10)\n","    config.VAL_EPOCHS = 1 \n","    config.LEARNING_RATE = 1e-4    # learning rate (default: 0.01)\n","    config.SEED = 42               # random seed (default: 42)\n","    # config.MAX_LEN = 512\n","    # config.SUMMARY_LEN = 150 \n","    config.MAX_LEN = 100\n","    config.SUMMARY_LEN = 20 \n","\n","    # Set random seeds and deterministic pytorch for reproducibility\n","    torch.manual_seed(config.SEED) # pytorch random seed\n","    np.random.seed(config.SEED) # numpy random seed\n","    torch.backends.cudnn.deterministic = True\n","\n","    # tokenzier for encoding the text\n","    # tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n","    tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n","\n","\n","    # Importing and Pre-Processing the domain data\n","    # Selecting the needed columns only. \n","    # Adding the summarzie text in front of the text. This is to format the dataset similar to how T5 model was trained for summarization task. \n","    df=df_cnn.copy()\n","    df= df[['story','highlights']]\n","    df['text'] = df['story']\n","    df['ctext'] = df['highlights']\n","    \n","    # Creation of Dataset and Dataloader\n","    # Defining the train size. So 80% of the data will be used for training and the rest will be used for validation. \n","    train_size = 0.8\n","    train_dataset=df.sample(frac=train_size,random_state = config.SEED)\n","    val_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n","    train_dataset = train_dataset.reset_index(drop=True)\n","\n","    print(\"FULL Dataset: {}\".format(df.shape))\n","    print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n","    print(\"TEST Dataset: {}\".format(val_dataset.shape))\n","\n","\n","    # Creating the Training and Validation dataset for further creation of Dataloader\n","    training_set = CustomDataset(train_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n","    val_set = CustomDataset(val_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n","\n","    # Defining the parameters for creation of dataloaders\n","    train_params = {\n","        'batch_size': config.TRAIN_BATCH_SIZE,\n","        'shuffle': True,\n","        'num_workers': 0\n","        }\n","\n","    val_params = {\n","        'batch_size': config.VALID_BATCH_SIZE,\n","        'shuffle': False,\n","        'num_workers': 0\n","        }\n","\n","    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n","    training_loader = DataLoader(training_set, **train_params)\n","    val_loader = DataLoader(val_set, **val_params)\n","\n","\n","    \n","    # Defining the model. We are using t5-base/bart-base model and added a Language model layer on top for generation of Summary. \n","    # Further this model is sent to device (GPU/TPU) for using the hardware.\n","    # model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n","    model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-base\")\n","\n","    model = model.to(device)\n","\n","    # Defining the optimizer that will be used to tune the weights of the network in the training session. \n","    optimizer = torch.optim.Adam(params =  model.parameters(), lr=config.LEARNING_RATE)\n","\n","    # Log metrics with wandb\n","    wandb.watch(model, log=\"all\")\n","    # Training loop\n","    print('Initiating Fine-Tuning for the model on our dataset')\n","\n","    for epoch in range(config.TRAIN_EPOCHS):\n","        train(epoch, tokenizer, model, device, training_loader, optimizer)\n","\n","\n","    # Validation loop and saving the resulting file with predictions and acutals in a dataframe.\n","    # Saving the dataframe as predictions.csv\n","    print('Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe')\n","    for epoch in range(config.VAL_EPOCHS):\n","        predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n","        final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n","        final_df.to_csv('./models/predictions.csv')\n","        print('Output Files generated for review')\n","\n","if __name__ == '__main__':\n","    main()"],"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/html":["Finishing last run (ID:n96vt74x) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<br/>Waiting for W&B process to finish, PID 466<br/>Program ended successfully."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bf9ba96fb6d54c22aaa4b665c237cd53","version_minor":0,"version_major":2},"text/plain":["VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["Find user logs for this run at: <code>/content/wandb/run-20201118_210021-n96vt74x/logs/debug.log</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["Find internal logs for this run at: <code>/content/wandb/run-20201118_210021-n96vt74x/logs/debug-internal.log</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["\n","                    <br/>Synced <strong style=\"color:#cdcd00\">silver-resonance-3</strong>: <a href=\"https://wandb.ai/thenatzzz/finetune_transformers_summarization/runs/n96vt74x\" target=\"_blank\">https://wandb.ai/thenatzzz/finetune_transformers_summarization/runs/n96vt74x</a><br/>\n","                "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["...Successfully finished last run (ID:n96vt74x). Initializing new run:<br/><br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["\n","                Tracking run with wandb version 0.10.11<br/>\n","                Syncing run <strong style=\"color:#cdcd00\">electric-frost-4</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n","                Project page: <a href=\"https://wandb.ai/thenatzzz/finetune_transformers_summarization\" target=\"_blank\">https://wandb.ai/thenatzzz/finetune_transformers_summarization</a><br/>\n","                Run page: <a href=\"https://wandb.ai/thenatzzz/finetune_transformers_summarization/runs/3smro010\" target=\"_blank\">https://wandb.ai/thenatzzz/finetune_transformers_summarization/runs/3smro010</a><br/>\n","                Run data is saved locally in <code>/content/wandb/run-20201118_210050-3smro010</code><br/><br/>\n","            "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["FULL Dataset: (301956, 4)\n","TRAIN Dataset: (241565, 4)\n","TEST Dataset: (60391, 4)\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"05bd7145af8c409097837567e4ef6c53","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=557941479.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"],"name":"stderr"},{"output_type":"stream","text":["Initiating Fine-Tuning for the model on our dataset\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/modeling_bart.py:1040: FutureWarning: The `lm_labels` argument is deprecated and will be removed in a future version, use `labels` instead.\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 0, Loss:  7.5591654777526855\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-d7358eab6e36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-20-d7358eab6e36>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-6fce8d5c0250>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, tokenizer, model, device, loader, optimizer)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# xm.optimizer_step(optimizer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zXBOfP0-0ZEi","executionInfo":{"status":"ok","timestamp":1605732812504,"user_tz":480,"elapsed":28077,"user":{"displayName":"Nattapat Juthaprachakul","photoUrl":"","userId":"05256019876215417123"}},"outputId":"4da9febe-a1a1-4f4c-f67a-a37088ba9342"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]}]}